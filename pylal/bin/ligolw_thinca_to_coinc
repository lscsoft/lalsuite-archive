#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
Translate thinca-style sngl_inspiral coincs to coinc-tables-style coincs.
"""


import itertools
import math
from optparse import OptionParser
import re
import sys


from glue import iterutils
from glue import lal
from glue import segments
from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import utils
from glue.ligolw.utils import ligolw_add
from glue.ligolw.utils import segments as ligolw_segments
from glue.ligolw.utils import process as ligolw_process
from pylal import llwapp
from pylal import ligolw_thinca
from pylal import ligolw_tisi
from pylal import SnglInspiralUtils


__author__ = "Kipp Cannon <kipp@gravity.phys.uwm.edu>"
__version__ = "$Revision$"[11:-2]
__date__ = "$Date$"[7:-2]


#
# =============================================================================
#
#                              Behavioural Tweaks
#
# =============================================================================
#


#
# Make sngl_inspiral table rows hashable so they can be used as dictionary
# keys and uniquified with set()
#


def __sngl_inspiral_hash__(self):
	# The things in this tuple must be the same things used in the
	# __cmp__() method for comparison (if two objects have different
	# hashes they must compare as not equal or stuff breaks), so make
	# sure to keep this updated if the choice of how to compare to
	# triggers changes.
	return hash((self.ifo, self.end_time, self.end_time_ns, self.mass1, self.mass2, self.search))


lsctables.SnglInspiral.__hash__ = __sngl_inspiral_hash__


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version = "%prog CVS $Id$",
		usage = "%prog [options] --output-prefix filename",
		description = "Converts thinca output files to coinc tables format.  Files whose names end in \".gz\" are assumed to be gzip-compressed.  If an ihope cache is provided then all second-stage thinca files found in the cache will be processed, grouped by run tag.  Otherwise, one or both of a zero-lag or time-slide thinca output file must be provided."
	)
	parser.add_option("--zero-lag-file", metavar = "filename", help = "Thinca output file containing zero lag triggers.")
	parser.add_option("--time-slide-file", metavar = "filename", help = "Thinca output file containing time slide triggers.")
	parser.add_option("--ihope-cache", metavar = "filename", help = "Get thinca files from this ihope cache.")
	parser.add_option("--instruments", metavar = "name[,name,...]", help = "Set the list of instruments.  Example H1,H2,L1.  If not provided, attempts will be made to deduce the instrument list either from --??-slide command line options in the process_params table or from the contents of the ifos column in the search_summary table.")
	parser.add_option("--output-prefix", metavar = "string", help = "Set the prefix string for output filenames (required).  Output filenames are constructed automatically, and are of the form \"PREFIX_blah_blah_blah.xml.gz\".  Hint:  can include a path.")
	parser.add_option("--veto-segments", metavar = "filename", help = "Load veto segments from this XML document.  See ligolw_segments for information on constructing such a document.")
	parser.add_option("--veto-segments-name", metavar = "string", help = "Set the name of the veto segments to use from the XML document.")
	parser.add_option("--effective-snr-factor", metavar = "float", type = "float", default = 250.0, help = "Set the effective SNR factor (default = 250.0).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	options, args = parser.parse_args()

	if options.ihope_cache:
		if options.zero_lag_file or options.time_slide_file:
			raise ValueError, "cannot specify --zero-lag-file or --time-slide-file with --ihope-cache"
		if options.verbose:
			print >>sys.stderr, "reading %s ..." % options.ihope_cache
		# find second thinca cache entries
		pattern = re.compile("THINCA(_SLIDE)?_SECOND_[^_]+_(?P<tag>.+)_CAT_(?P<cat>[\d]+)_VETO")
		zero_lag_urls = {}
		slide_urls = {}
		for c in map(lal.CacheEntry, file(options.ihope_cache)):
			category = pattern.match(c.description)
			if category is None:
				continue
			category = category.groups()
			if category[0] is None:
				# zero-lag URL
				try:
					zero_lag_urls[category[1:]].append(c)
				except KeyError:
					zero_lag_urls[category[1:]] = [c]
			else:
				# slide URL
				try:
					slide_urls[category[1:]].append(c)
				except KeyError:
					slide_urls[category[1:]] = [c]
		# extract URLs.  sorting cache entries puts them in time
		# order
		zero_lag_urls = dict((key, [c.url for c in sorted(cache)]) for key, cache in zero_lag_urls.items())
		slide_urls = dict((key, [c.url for c in sorted(cache)]) for key, cache in slide_urls.items())
		# pad lists with None's so zero-lag and slide lists exist
		# for all the same categories and have the same number of
		# entries
		for key in set(zero_lag_urls.keys()) | set(slide_urls.keys()):
			if key not in slide_urls:
				# must be in zero-lag
				slide_urls[key] = [None,] * len(zero_lag_urls[key])
			elif key not in zero_lag_urls:
				# must be in slides
				zero_lag_urls[key] = [None,] * len(slide_urls[key])
			# must be in both, make sure lengths are the same
			elif len(zero_lag_urls[key]) > len(slide_urls[key]):
				slide_urls[key] += [None,] * (len(zero_lag_urls[key]) - len(slide_urls[key]))
			elif len(zero_lag_urls[key]) < len(slide_urls[key]):
				zero_lag_urls[key] += [None,] * (len(slide_urls[key]) - len(zero_lag_urls[key]))
		# match up each zero-lag url with corresponding slides urls
		# in each category
		categories = dict((key, zip(zero_lag_urls[key], slide_urls[key])) for key in zero_lag_urls.keys())
	elif options.zero_lag_file or options.time_slide_file:
		categories = {(None, None): [(options.zero_lag_file, options.time_slide_file)]}
	else:
		raise ValueError, "must specify --ihope-cache or at least one of --zero-lag-file and --time-slide-file"
	if not options.output_prefix:
		raise ValueError, "must specify --output-prefix"
	if options.veto_segments and not options.veto_segments_name:
		raise ValueError, "must specify --veto-segments-name if --veto-sements is specified"
	if options.instruments:
		options.instruments = set(options.instruments.split(","))

	if args:
		raise ValueError, "extraneous command line arguments specified"

	return options, categories


#
# =============================================================================
#
#                             Process Information
#
# =============================================================================
#


#
# create and initialize this job's row in the process table
#


def initialize_process(xmldoc, comment = u""):
	return llwapp.append_process(xmldoc, program = u"ligolw_thinca_to_coinc", version = __version__, cvs_repository = u"lscsoft", cvs_entry_time = __date__, comment = comment)


#
# record command line arguments
#


def set_process_params(xmldoc, process, options):
	params = []
	if options.zero_lag_file:
		params.append((u"--zero-lag-file", u"lstring", options.zero_lag_file))
	if options.time_slide_file:
		params.append((u"--time-slide-file", u"lstring", options.time_slide_file))
	if options.ihope_cache:
		params.append((u"--ihope-cache", u"lstring", options.ihope_cache))
	if options.instruments:
		params.append((u"--instruments", u"lstring", ",".join(options.instruments)))
	if options.veto_segments:
		params.append((u"--veto-segments", u"lstring", options.veto_segments))
	if options.veto_segments_name:
		params.append((u"--veto-segments-name", u"lstring", options.veto_segments_name))
	if options.effective_snr_factor:
		params.append((u"--effective-snr-factor", u"lstring", options.effective_snr_factor))
	params.append((u"--output-prefix", u"lstring", options.output_prefix))

	ligolw_process.append_process_params(xmldoc, process, params)

	return xmldoc


#
# =============================================================================
#
#                               Document Fix-Up
#
# =============================================================================
#


#
# add missing ID columns.  this is needed because the cbc 2yr tags are on
# versions of LAL that don't have these columns in the tables.  current
# versions of LAL do.
#


def add_missing_id_columns(xmldoc, verbose = False):
	if verbose:
		print >>sys.stderr, "adding any missing ID columns ...",

	#
	# fix summ_value and search_summ_vars tables
	#

	for name in (lsctables.SummValueTable.tableName, lsctables.SearchSummVarsTable.tableName):
		try:
			tbl = table.get_table(xmldoc, name)
			tbl.appendColumn(tbl.next_id.column_name)
		except ValueError:
			# document doesn't have this table or already has
			# ID column
			continue
		if verbose:
			print >>sys.stderr, name,
		for row in tbl:
			setattr(row, tbl.next_id.column_name, tbl.get_next_id())

	#
	# done
	#

	if verbose:
		print >>sys.stderr


#
# Fix ifos columns by turning strings like "H1H2L1" into strings like
# "H1,H2,L1".
#


def fix_ifos_columns(xmldoc, verbose = False):
	if verbose:
		print >>sys.stderr, "fixing ifos columns ...",

	#
	# fix process and search_summary tables
	#

	for table_name in (lsctables.ProcessTable.tableName, lsctables.SearchSummaryTable.tableName):
		try:
			tbl = table.get_table(xmldoc, table_name)
		except ValueError:
			# document doesn't have this table
			continue
		if options.verbose:
			print >>sys.stderr, table_name,
		for row in tbl:
			row.set_ifos(row.get_ifos())

	#
	# done
	#

	if verbose:
		print >>sys.stderr


#
# =============================================================================
#
#                          Populate time_slide Table
#
# =============================================================================
#


def get_thinca_process_ids(xmldoc):
	return table.get_table(xmldoc, lsctables.ProcessTable.tableName).get_ids_by_program("thinca")


def get_search_summary_instruments(xmldoc, thinca_process_ids):
	"""
	Extract the list of analyzed instruments from the thinca entries in
	the search summary table.
	"""
	search_summary_instruments = set(frozenset(row.get_ifos()) for row in table.get_table(xmldoc, lsctables.SearchSummaryTable.tableName) if row.process_id in thinca_process_ids)
	if len(search_summary_instruments) < 1:
		raise ValueError, "cannot find entries for thinca jobs in search_summary table"
	if len(search_summary_instruments) > 1:
		raise ValueError, "search_summary table contains entries for thinca jobs from more than 1 unique set of instruments:  found %s" % ", ".join(search_summary_instruments)
	return search_summary_instruments.pop()


def populate_thinca_time_slide_table(xmldoc, process, instruments = None, verbose = False):
	"""
	Reconstruct the list of time slides from lalapps_thinca's command
	line arguments.
	"""
	if verbose:
		print >>sys.stderr, "populating thinca time_slide table ...",

	#
	# find the time_slide table or add one if needed
	#

	try:
		time_slide_table = table.get_table(xmldoc, lsctables.TimeSlideTable.tableName)
	except ValueError:
		time_slide_table = lsctables.New(lsctables.TimeSlideTable)
		xmldoc.childNodes[0].appendChild(time_slide_table)

	#
	# move existing time_slide IDs out of the way
	#

	# find the lowest unused ID not less than 10000 and set next_id to
	# that value
	time_slide_table.sync_next_id()
	if time_slide_table.next_id < type(time_slide_table.next_id)(10000):
		time_slide_table.set_next_id(type(time_slide_table.next_id)(10000))
	# use the updateKeyMapping method to re-label all existing
	# time_slide IDs and record the old-->new mapping
	mapping = {}
	time_slide_table.updateKeyMapping(mapping)
	# apply the mapping to all other tables in the document to update
	# any references to existing time_slide IDs
	for tbl in xmldoc.getElementsByTagName(time_slide_table.tagName):
		tbl.applyKeyMapping(mapping)

	#
	# get process_ids for all thinca jobs, and for thinca jobs that
	# were run with a --num-slides command line option
	#

	thinca_process_ids = get_thinca_process_ids(xmldoc)
	slides_process_ids = thinca_process_ids & set(row.process_id for row in table.get_table(xmldoc, lsctables.ProcessParamsTable.tableName) if row.param == u"--num-slides")

	if len(thinca_process_ids - slides_process_ids) > 1:
		raise ValueError, "document contains more than 1 zero-lag thinca job"
	if len(slides_process_ids) > 1:
		raise ValueError, "document contains more than 1 non-zero-lag thinca job"

	#
	# get the list of analyzed instruments from the search_summary
	# table
	#

	search_summary_instruments = get_search_summary_instruments(xmldoc, thinca_process_ids)

	#
	# if a set of instruments has been provided by the user require it
	# to be a superset of the instruments named in the search_summary
	# table, either way the union of the two is the required instrument
	# list
	#

	if instruments is None:
		required_instruments = search_summary_instruments
	elif search_summary_instruments.issubset(instruments):
		required_instruments = instruments
	else:
		raise ValueError, "search_summary table named instrument(s) %s that were not named on the command line" % ", ".join(search_summary_instruments - instruments)

	#
	# identify lalapps_thinca's time slides
	#

	if not slides_process_ids:
		#
		# just zero-lag jobs.  synthesize an all-zero vector from
		# the instruments we've retrieved from the search_summary
		# table, and set num_slides to 0
		#

		offset_vector = dict((instrument, 0.0) for instrument in required_instruments)
		num_slides = 0

	else:
		#
		# construct the offset vector from the --??-slide command
		# line options, and extact num_slides from the --num-slides
		# command line option
		#

		num_slides = None
		offset_vector = {}
		slide_option_pattern = re.compile("--(?P<ifo>[a-zA-Z][0-9])-slide")

		for row in table.get_table(xmldoc, lsctables.ProcessParamsTable.tableName):
			if row.process_id in slides_process_ids:
				if row.param == u"--num-slides":
					num_slides = row.get_pyvalue()
					if num_slides < 0:
						raise ValueError, "invalid --num-slides '%s' for process '%s'" % (row.value, row.process_id)
				else:
					match = re.search(slide_option_pattern, row.param)
					if match is not None:
						offset_vector[match.groupdict()["ifo"].upper()] = row.get_pyvalue()

		#
		# confirm that the offset vector contains offsets for all
		# required instruments
		#

		if not required_instruments.issubset(set(offset_vector.keys())):
			missing_instruments = set(offset_vector.keys()) - required_instruments
			raise ValueError, "no option(s) %s in process_params table for instrument(s) %s in search_summary table" % (", ".join(["--%s-slide" % instrument.lower() for instrument in missing_instruments]), ", ".join(missing_instruments))

		#
		# if the user has provided an explicit list of instruments,
		# remove instruments from the offset vector that were not
		# named by the user
		#

		if instruments is not None:
			for instrument in set(offset_vector.keys()) - instruments:
				del offset_vector[instrument]

	#
	# build the time slide vectors for the cases when all instruments
	# are on.  these must be given time_slide IDs that match the slide
	# number component of the old-style event_id
	#

	def ids(id_class, num_slides):
		for n in range(-num_slides, +num_slides + 1):
			if n < 0:
				yield id_class(5000 - n)
			else:
				yield id_class(n)

	n = 0
	for id, offset_vector in zip(ids(type(time_slide_table.next_id), num_slides), ligolw_tisi.Inspiral_Num_Slides_Iter(num_slides, offset_vector)):
		n += 1
		for row in ligolw_tisi.RowsFromOffsetDict(offset_vector, id, process):
			time_slide_table.append(row)

	#
	# done
	#

	if verbose:
		print >>sys.stderr, "added %d time slide vectors" % n


def depopulate_time_slide_table(xmldoc, verbose = False):
	"""
	Search for and remove duplicate time slide definitions from the
	time_slide table.
	"""
	if verbose:
		print >>sys.stderr, "depopulating time_slides ...",

	#
	# find the time_slide table
	#

	time_slide_table = table.get_table(xmldoc, lsctables.TimeSlideTable.tableName)

	length_before = len(set(time_slide_table.getColumnByName("time_slide_id")))

	#
	# translate time_slide table into a dictionary, and identify
	# redundant IDs
	#
	# NOTE:  the time slide vector comparison code treats {"H1": 0,
	# "L1": 5} and {"H1": 10, "L1": 15} as identical vectors because
	# the relative offsets are identical.  in the inspiral pipeline,
	# these are potentially different time slides because one of the
	# two can result in the pair of triggers straddling a ring boundary
	# while the other does not, so a trigger pair can be coincident for
	# one of these vectors but not the other.  this should never be an
	# issue because the pipeline also has the limitation of only being
	# able to apply vectors that are all multiples of a fixed basis
	# vector --- if two instruments have the same relative offsets in
	# two vectors then they must also have the same absolute offsets in
	# those vectors, so comparing by relative offsets yields the same
	# set of redundant vectors that a comparison by absolute offsets
	# would yield.  the use of the time_slides_vacuum() function here
	# is correct, but one should keep the reason why in mind if one is
	# tempted to copy-and-paste this code elsewhere.
	#

	mapping = ligolw_tisi.time_slides_vacuum(time_slide_table.as_dict())

	#
	# remove rows corresponding to redundant IDs
	#

	for i in xrange(len(time_slide_table) - 1, -1, -1):
		if time_slide_table[i].time_slide_id in mapping:
			del time_slide_table[i]

	#
	# reassign time_slide IDs in the rest of the document
	#

	for tbl in xmldoc.getElementsByTagName(time_slide_table.tagName):
		tbl.applyKeyMapping(mapping)

	#
	# done
	#

	if verbose:
		length_after = len(set(time_slide_table.getColumnByName("time_slide_id")))
		print >>sys.stderr, "removed %d redundant time slide vectors, %d remaining" % (length_before - length_after, length_after)


#
# =============================================================================
#
#       Populate coinc_event, coinc_event_map, and coinc_inspiral Tables
#
# =============================================================================
#


#
# retrieve the ring boundaries
#


def retrieve_ring_boundaries(xmldoc):
	#
	# grab the segment list for any instrument selected at random (they
	# are all the same)
	#

	rings = llwapp.segmentlistdict_fromsearchsummary(xmldoc, program = "thinca").popitem()[1]

	#
	# because the input often contains two thinca jobs the rings might
	# be duplicated;  use set() to uniqueify them then sort them.
	#

	rings = segments.segmentlist(set(rings))
	rings.sort()

	#
	# check that the (sorted) rings are non-intersecting
	#

	for i in range(len(rings) - 1):
		if rings[i].interesects(rings[i + 1]):
			raise ValueError, "non-disjoint thinca rings detected in search_summary table"

	#
	# done
	#

	return rings


#
# For sngl_inspiral <--> sngl_inspiral coincidences
#


def populate_coinc_event_sngls(xmldoc, process, effective_snr_factor = 250.0, verbose = False):
	if verbose:
		print >>sys.stderr, "constructing coincs ...",

	#
	# retrieve the ring boundaries
	#

	rings = retrieve_ring_boundaries(xmldoc)

	#
	# get the list of analyzed instruments from the search_summary
	# table
	#

	on_instruments = get_search_summary_instruments(xmldoc, get_thinca_process_ids(xmldoc))

	#
	# find the coinc_definer_id for sngl_inspiral <--> sngl_inspiral
	# coincidences, or create it if needed
	#

	coinc_def_id = llwapp.get_coinc_def_id(xmldoc, ligolw_thinca.InspiralCoincDef.search, ligolw_thinca.InspiralCoincDef.search_coinc_type, create_new = True, description = ligolw_thinca.InspiralCoincDef.description)

	#
	# find the coinc_event table or create one if needed
	#

	try:
		coinc_event_table = table.get_table(xmldoc, lsctables.CoincTable.tableName)
	except ValueError:
		coinc_event_table = xmldoc.childNodes[0].appendChild(lsctables.New(lsctables.CoincTable))

	#
	# synchronize the coinc_event table's ID generator with any
	# pre-existing rows
	#

	coinc_event_table.sync_next_id()

	#
	# find the coinc_event_map table or create one if needed
	#

	try:
		coinc_event_map_table = table.get_table(xmldoc, lsctables.CoincMapTable.tableName)
	except ValueError:
		coinc_event_map_table = xmldoc.childNodes[0].appendChild(lsctables.New(lsctables.CoincMapTable))

	#
	# find the coinc_inspiral table or create one if needed
	#

	try:
		coinc_inspiral_table = table.get_table(xmldoc, lsctables.CoincInspiralTable.tableName)
	except ValueError:
		coinc_inspiral_table = xmldoc.childNodes[0].appendChild(lsctables.New(lsctables.CoincInspiralTable))

	#
	# find the sngl_inspiral table
	#

	try:
		sngl_inspiral_table = table.get_table(xmldoc, lsctables.SnglInspiralTable.tableName)
	except ValueError:
		# no sngl_inspiral table --> no-op
		if verbose:
			print >>sys.stderr, "cannot find sngl_inspiral table"
		return

	#
	# find the time_slide table, and convert it to a dictionary of
	# offset vectors
	#

	time_slide_table = table.get_table(xmldoc, lsctables.TimeSlideTable.tableName)
	time_slides = time_slide_table.as_dict()

	#
	# iterate over reconstructed coincs
	#

	sngl_inspiral_table.sort(lambda a, b: cmp(a.event_id, b.event_id))
	for event_id, events in itertools.groupby(sngl_inspiral_table, lambda row: row.event_id):
		#
		# alphabetize events by instrument
		#

		events = tuple(sorted(events, lambda a, b: cmp(a.ifo, b.ifo)))
		if len(events) < 2:
			# not a coincidence, just a single.  assign a new,
			# unique, event_id and continue.
			for event in events:
				event.event_id = sngl_inspiral_table.get_next_id()
			continue

		#
		# build a coinc_event
		#

		coinc = coinc_event_table.RowType()
		coinc.process_id = process.process_id
		coinc.coinc_event_id = coinc_event_table.get_next_id()
		coinc.coinc_def_id = coinc_def_id
		coinc.nevents = len(events)
		coinc.set_instruments(on_instruments)
		coinc.likelihood = None
		coinc_event_table.append(coinc)

		#
		# construct the time_slide_id from the "slide number" at
		# index 1 in the tuple returned by get_id_parts().  all
		# events in the coinc have the same ID at this stage so it
		# doesn't matter which event's ID we use.
		#

		coinc.time_slide_id = type(time_slide_table.next_id)(events[0].get_id_parts()[1])

		#
		# link events to coinc with coinc_event_map rows.  the
		# inspiral triggers are assigned new, unique, event IDs
		# here.
		#

		for event in events:
			coincmap = coinc_event_map_table.RowType()
			coincmap.coinc_event_id = coinc.coinc_event_id
			coincmap.event_id = event.event_id = sngl_inspiral_table.get_next_id()
			coincmap.table_name = coincmap.event_id.table_name
			coinc_event_map_table.append(coincmap)

		#
		# populate coinc_inspiral table with coinc summary:
		#
		# - end_time is the end time of the first trigger in
		#   alphabetical order by instrument (!?) time-shifted
		#   according to the coinc's offset vector
		# - mass is average of mtotals
		# - snr is root-sum-square of SNRs
		# - false-alarm rate is blank
		#

		coinc_inspiral = lsctables.CoincInspiral()
		coinc_inspiral.coinc_event_id = coinc.coinc_event_id
		coinc_inspiral.set_ifos(str(event.ifo) for event in events)
		#coinc_inspiral.mass = sum(event.mchirp for event in events) / len(events)
		coinc_inspiral.mass = sum(event.mass1 + event.mass2 for event in events) / len(events)
		coinc_inspiral.snr = math.sqrt(sum(event.get_effective_snr(fac = effective_snr_factor)**2 for event in events))
		coinc_inspiral.false_alarm_rate = None

		#
		# the time of the coinc = the end time of the first trigger
		# in the coinc in alphabetical order by instrument
		#

		coinc_time = events[0].get_end()

		#
		# which ring is it in?
		#

		ring = rings[rings.find(coinc_time)]

		#
		# the amount by which to slide it = the offset recorded in
		# the time_slide table for the instrument from which the
		# coinc's time has been taken
		#

		offset = time_slides[coinc.time_slide_id][events[0].ifo]

		#
		# slide the coinc's time on the ring.
		#

		coinc_inspiral.set_end(SnglInspiralUtils.slideTimeOnRing(coinc_time, offset, ring))

		coinc_inspiral_table.append(coinc_inspiral)

	#
	# done
	#

	if verbose:
		print >>sys.stderr, "constructed %d coincs" % len(coinc_event_table)


#
# =============================================================================
#
#                        Depopulate sngl_inspiral Table
#
# =============================================================================
#


def depopulate_sngl_inspiral(xmldoc, verbose = False):
	if verbose:
		print >>sys.stderr, "depopulating sngl_inspirals ...",

	#
	# find the sngl_inspiral table
	#

	try:
		sngl_inspiral_table = table.get_table(xmldoc, lsctables.SnglInspiralTable.tableName)
	except ValueError:
		# no sngl_inspiral table --> no-op
		if verbose:
			print >>sys.stderr, "cannot find sngl_inspiral table"
		return

	length_before = len(sngl_inspiral_table)

	#
	# delete duplicates, recording replacement event_ids.  this relies
	# on the SnglInspiral class' __eq__() and __hash__() methods to
	# define when two triggers are the same
	#

	trigger_to_id_index = {}
	mapping = {}
	for i in xrange(len(sngl_inspiral_table) - 1, -1, -1):
		trigger = sngl_inspiral_table[i]
		if trigger in trigger_to_id_index:
			mapping[trigger.event_id] = trigger_to_id_index[trigger]
			del sngl_inspiral_table[i]
		else:
			trigger_to_id_index[trigger] = trigger.event_id

	#
	# update IDs in other tables
	#

	for tbl in xmldoc.getElementsByTagName(sngl_inspiral_table.tagName):
		tbl.applyKeyMapping(mapping)

	#
	# done
	#

	if verbose:
		print >>sys.stderr, "removed %d redundant sngl_inspiral events, %d remaining" % (length_before - len(sngl_inspiral_table), len(sngl_inspiral_table))


#
# =============================================================================
#
#                                 Apply Vetoes
#
# =============================================================================
#


def apply_vetoes(xmldoc, veto_segments, process, verbose = False):
	if verbose:
		print >>sys.stderr, "applying vetoes ..."

	#
	# find the tables we'll need
	#

	try:
		sngl_inspiral_table = table.get_table(xmldoc, lsctables.SnglInspiralTable.tableName)
	except ValueError:
		# no sngl_inspiral table --> no-op
		if verbose:
			print >>sys.stderr, "\tcannot find sngl_inspiral table"
		return
	coinc_table = table.get_table(xmldoc, lsctables.CoincTable.tableName)
	coinc_map_table = table.get_table(xmldoc, lsctables.CoincMapTable.tableName)
	coinc_inspiral_table = table.get_table(xmldoc, lsctables.CoincInspiralTable.tableName)
	time_slide_table = table.get_table(xmldoc, lsctables.TimeSlideTable.tableName)

	#
	# turn the time slide table into a dictionary
	#

	if verbose:
		print >>sys.stderr, "\tindexing time_slide table and computing segment lists ..."
	time_slides = time_slide_table.as_dict()

	#
	# retrieve the ring boundaries
	#

	rings = retrieve_ring_boundaries(xmldoc)

	#
	# performance assist:  remove veto segments that don't fall in any
	# of the rings, then remove veto segment lists that are empty
	#

	coalesced_rings = segments.segmentlist(rings).coalesce()
	veto_segments = segments.segmentlistdict((instrument, seglist & coalesced_rings) for (instrument, seglist) in veto_segments.items() if seglist.intersects(coalesced_rings))
	if not veto_segments:
		if verbose:
			print >>sys.stderr, "\tno vetos were on during the times spanned by this document"
		return

	#
	# create the coinc_event_id --> sngl_inspiral index, and
	# coinc_event_id --> coinc_inspiral index
	#

	if verbose:
		print >>sys.stderr, "\tindexing coinc tables ..."
	index = dict((row.event_id, row) for row in sngl_inspiral_table)

	coinc_map_table.sort(lambda a, b: cmp(a.coinc_event_id, b.coinc_event_id))
	index = dict(
		(coinc_event_id, tuple(index[row.event_id] for row in rows))
		for coinc_event_id, rows in itertools.groupby(
			(row for row in coinc_map_table if row.table_name == "sngl_inspiral"),
			lambda row: row.coinc_event_id
		)
	)

	coinc_inspirals = dict((row.coinc_event_id, row) for row in coinc_inspiral_table)

	#
	# iterate over coincs
	#

	if verbose:
		print >>sys.stderr, "\tchecking for coincs during vetoed times ..."

	cached_vetoes = {}
	N = len(coinc_table) / 1000 or 1

	for n, coinc_event in enumerate(coinc_table):
		if verbose and not (n % N):
			print >>sys.stderr, "\t\t%.1f%%\r" % (100.0 * n / len(coinc_table)),

		if coinc_event.coinc_event_id not in index:
			#
			# not a coinc we care about
			#

			continue

		#
		# retrieve the time slide vector
		#

		offset_vector = time_slides[coinc_event.time_slide_id]

		#
		# compare the instruments participating in the coinc to the
		# instruments named in the time slide vector
		#

		expected_instruments = set(offset_vector.keys())
		found_instruments = set(event.ifo for event in index[coinc_event.coinc_event_id])

		if found_instruments == expected_instruments:
			#
			# all instruments contributed to this coinc:
			# nothing more to check
			#

			continue

		if not found_instruments.issubset(expected_instruments):
			raise ValueError, "coinc '%s' has instrument(s) %s that are not in time slide vector '%s' (%s)" % (coinc_event.coinc_event_id, ", ".join(found_instruments - expected_instruments), coinc_event.time_slide_id, ", ".join(["%s=%g" % item for item in offset_vector.items()]))

		#
		# if we get here, the time shift vector names instruments
		# that did not particicpate in the coinc.  check to see
		# which instruments were on at the time
		#

		#
		# the slid time of the coinc
		#

		coinc_time = coinc_inspirals[coinc_event.coinc_event_id].get_end()

		#
		# which ring is it in?  note:  although the time recorded
		# for the coinc is its time after offsets are applied to
		# triggers, that time is always in the same ring as the
		# original time of the trigger from which the coinc's time
		# has been taken
		#

		ring = rings[rings.find(coinc_time)]

		#
		# slide the veto segments on that ring, cache the results
		# to avoid recalculation
		#

		try:
			vetoes = cached_vetoes[(ring, coinc_event.time_slide_id)]
		except KeyError:
			vetoes = cached_vetoes[(ring, coinc_event.time_slide_id)] = SnglInspiralUtils.slideSegListDictOnRing(ring, veto_segments, offset_vector)

		#
		# set the coinc's instruments to just those that were on at
		# the time of the coinc.  note that because the assignment
		# of a single time to a coinc is a poorly-defined
		# procedure, it can come to pass that a coinc is said to
		# have occured at a time when the instruments that
		# participate in it are vetoed.  we sweep this under the
		# rug by adding to the list of instruments that are on at
		# least those that participated in the coinc.
		#

		on_instruments = set(instrument for instrument in offset_vector.keys() if instrument not in vetoes or coinc_time not in vetoes[instrument])
		coinc_event.set_instruments(on_instruments | found_instruments)

	if verbose:
		print >>sys.stderr, "\t\t100.0%"


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


options, url_categories = parse_command_line()

if options.veto_segments:
	veto_segments = ligolw_segments.segmenttable_get_by_name(utils.load_filename(options.veto_segments, gz = (options.veto_segments or "stdin").endswith(".gz"), verbose = options.verbose), options.veto_segments_name).coalesce()


for (tag, veto_category), url_pairs in url_categories.items():
	output_template = "%s%s%s_%%0%dd.xml.gz" % (options.output_prefix, tag and ("_%s" % tag) or "", veto_category and ("_CAT_%s" % veto_category) or "", int(math.floor(math.log10(len(url_pairs) or 1) + 1)))
	for n, url_pair in enumerate(url_pairs):
		lsctables.SnglInspiralTable.next_id = None
		xmldoc = ligolw_add.ligolw_add(ligolw.Document(), [url for url in url_pair if url], verbose = options.verbose)
		lsctables.SnglInspiralTable.next_id = lsctables.SnglInspiralID(0)

		process = initialize_process(xmldoc)
		set_process_params(xmldoc, process, options)

		add_missing_id_columns(xmldoc, verbose = options.verbose)
		# FIXME:  uncomment this after Kipp talks people into it.
		#fix_ifos_columns(xmldoc, verbose = options.verbose)

		populate_thinca_time_slide_table(xmldoc, process, instruments = options.instruments, verbose = options.verbose)

		populate_coinc_event_sngls(xmldoc, process, effective_snr_factor = options.effective_snr_factor, verbose = options.verbose)

		depopulate_sngl_inspiral(xmldoc, verbose = options.verbose)

		if options.veto_segments:
			apply_vetoes(xmldoc, veto_segments, process, verbose = options.verbose)
		elif options.verbose:
			print >>sys.stderr, "no vetoes applied"

		depopulate_time_slide_table(xmldoc, verbose = options.verbose)

		llwapp.set_process_end_time(process)

		output = output_template % n
		utils.write_filename(xmldoc, output, verbose = options.verbose, gz = (output or "stdout").endswith(".gz"))
