#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
Build a LAL cache from a list of LIGO LW XML files containing search
summary tables.
"""


import glob
from optparse import OptionParser
import os
import sys
# Python 2.3 compatibility
try:
	set
except NameError:
	from sets import Set as set


from glue.lal import CacheEntry
from glue.ligolw import ligolw
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import utils


__author__ = "Kipp Cannon <kcannon@ligo.caltech.edu>"
__version__ = "$Revision$"[11:-2]
__date__ = "$Date$"[7:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version = "%prog CVS $Id$",
		usage = "%prog [options] filenames ...",
		description = "Generates a LAL format cache file describing a collection of LIGO light-weight XML files.  The cache is constructed by parsing the search_summary table in each file to extract the instruments and time each file spans.  To allow long file lists to be processed, the filenames are interpreted as shell patterns (wildcard expansion is performed)."
	)
	parser.add_option("--description", metavar = "string", help = "Set all descriptions to this string.")
	parser.add_option("--observatory", metavar = "string", help = "Set all observatories to this string.")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	parser.add_option("-o", "--output", metavar = "filename", help = "Set output file (default = stdout).")
	parser.add_option("-p", "--program", metavar = "name", help = "Obtain start and durations from search summary entries corresponding to the given program (default = whatever is there).")
	options, filenames = parser.parse_args()

	if options.output:
		options.output = file(options.output, "w")
	else:
		options.output = sys.stdout

	if filenames is None:
		filenames = []
	else:
		filenames = [filename for g in filenames for filename in glob.glob(g)]

	return options, filenames


#
# =============================================================================
#
#                                    Input
#
# =============================================================================
#


class ContentHandler(ligolw.PartialLIGOLWContentHandler):
	def __init__(self, xmldoc):
		def element_filter(name, attrs):
			"""
			Return True if name & attrs describe a search summary table or a
			process table.
			"""
			return lsctables.IsTableProperties(lsctables.SearchSummaryTable, name, attrs) or lsctables.IsTableProperties(lsctables.ProcessTable, name, attrs)
		ligolw.PartialLIGOLWContentHandler.__init__(self, xmldoc, element_filter)


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


options, filenames = parse_command_line()


for n, filename in enumerate(filenames):
	# load document and extract search summary table
	if options.verbose:
		print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
	xmldoc = utils.load_filename(filename, verbose = options.verbose, gz = filename.endswith(".gz"), contenthandler = ContentHandler)
	searchsumm = table.get_table(xmldoc, lsctables.SearchSummaryTable.tableName)

	# extract process_ids for the requested program
	if options.program is not None:
		process_ids = table.get_table(xmldoc, lsctables.ProcessTable.tableName).get_ids_by_program(options.program)
	else:
		process_ids = None

	# extract segment lists
	seglists = searchsumm.get_out_segmentlistdict(process_ids).coalesce()
	if not seglists:
		raise ValueError, "%s: no matching rows found in search summary table" % filename

	# extract observatory
	observatory = options.observatory.strip() or "+".join(sorted(seglists.keys()))

	# extract description
	if options.description:
		description = options.description
	else:
		if process_ids is None:
			description = set(searchsumm.getColumnByName("comment"))
		else:
			description = set([row.comment for row in searchsumm if row.process_id in process_ids])
		if len(description) < 1:
			raise ValueError, "%s: no matching rows found in search summary table" % filename
		if len(description) > 1:
			raise ValueError, "%s: comments in matching rows of search summary table are not identical" % filename
		description = description.pop().strip() or None

	# set URL
	url = "file://localhost" + os.path.abspath(filename)

	# write cache entry
	print >>options.output, str(CacheEntry(observatory, description, seglists.extent_all(), url))

	# allow garbage collection
	xmldoc.unlink()
