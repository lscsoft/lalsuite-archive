#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2006  Kipp C. Cannon
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
String cusp search final output rendering tool.
"""


import bisect
from optparse import OptionParser
import math
from matplotlib import patches
import numpy
from scipy import interpolate
from scipy import optimize
try:
	import sqlite3
except ImportError:
	# pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3
import sys


from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import dbtables
from glue.ligolw import utils
from glue import segments
from pylal import rate
from pylal import SimBurstUtils
from pylal import SnglBurstUtils
from pylal.xlal.datatypes.ligotimegps import LIGOTimeGPS


SnglBurstUtils.matplotlib.rcParams.update({
	"font.size": 10.0,
	"axes.titlesize": 10.0,
	"axes.labelsize": 10.0,
	"xtick.labelsize": 8.0,
	"ytick.labelsize": 8.0,
	"legend.fontsize": 8.0
})


lsctables.LIGOTimeGPS = LIGOTimeGPS


__author__ = "Kipp Cannon <kcannon@ligo.caltech.edu>"
__version__ = "$Revision$"[11:-2]
__date__ = "$Date$"[7:-2]


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version="%prog CVS $Id$",
		usage = "%prog [options] [file ...]",
		description = "%prog performs the final, summary, stages of the upper-limit string cusp search.  Input files ending in \".gz\" are assumed to be gzip-compressed, and if no files are specified on the command line then input is read from stdin."
	)
	parser.add_option("-a", "--amplitude-factor", metavar = "factor", type = "float", default = 1e-20, help = "Multiple amplitudes in XML files by this amount (default = 1e-20).")
	parser.add_option("--cal-uncertainty", metavar = "fraction", type = "float", default = 0.0, help = "Set the fractional uncertainty in amplitude due to calibration uncertainty (eg. 0.08).")
	parser.add_option("-i", "--injections", action = "store_true", help = "Generate efficiency plots from the output files of an injection run.")
	parser.add_option("--injections-bin-size", metavar = "bins", type = "float", default = 16.7, help = "Set bin width for injection efficiency curves.")
	parser.add_option("--loudest-survivor", metavar = "instrument=amplitude", help = "Set amplitude of loudest zero-lag non-injection survivor for efficiency plots.")
	parser.add_option("--image-formats", metavar = "ext[,ext,...]", default = "png,pdf", help = "Set list of graphics formats to produce by providing a comma-delimited list of the filename extensions.  Default = \"png,pdf\"")
	parser.add_option("-l", "--live-time-program", metavar = "program", default = "StringSearch", help = "Set the name, as it appears in the process table, of the program whose search summary entries define the search live time (default = StringSearch).")
	parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	options, filenames = parser.parse_args()

	if options.injections:
		if options.loudest_survivor is None:
			raise ValueError, "must set --loudest-survivor with --injections"
		options.loudest_survivor = options.loudest_survivor.split("=")
		options.loudest_survivor = {options.loudest_survivor[0]: abs(float(options.loudest_survivor[1]))}

	if options.cal_uncertainty is None:
		raise ValueError, "must set --cal-uncertainty"

	options.image_formats = options.image_formats.split(",")

	return options, (filenames or [None])


#
# =============================================================================
#
#                              Zero-Lag Survivors
#
# =============================================================================
#


class Survivors(object):
	def __init__(self):
		self.xmldoc = None

	def add_contents(self, contents):
		if self.xmldoc is None:
			self.xmldoc = ligolw.Document()
			self.xmldoc.appendChild(ligolw.LIGO_LW())
			self.sngl_burst_table = lsctables.New(lsctables.SnglBurstTable, contents.sngl_burst_table.columnnames)
			self.xmldoc.childNodes[0].appendChild(self.sngl_burst_table)

		for values in contents.connection.cursor().execute("""
SELECT
	sngl_burst.*
FROM
	coinc_event
	JOIN coinc_event_map ON (
		coinc_event_map.coinc_event_id == coinc_event.coinc_event_id
	)
	JOIN sngl_burst ON (
		coinc_event_map.table_name == 'sngl_burst'
		AND coinc_event_map.event_id == sngl_burst.event_id
	)
WHERE
	coinc_event.coinc_def_id == ?
	AND NOT EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
		""", (contents.bb_definer_id,)):
			self.sngl_burst_table.append(contents.sngl_burst_table.row_from_cols(values))

	def finish(self, filename, verbose = False):
		self.sngl_burst_table.sort(lambda a, b: cmp((a.ifo, abs(a.amplitude)), (b.ifo, abs(b.amplitude))))
		utils.write_filename(self.xmldoc, filename, verbose = verbose, gz = (filename or "stdout").endswith(".gz"))


#
# =============================================================================
#
#                              Rate vs. Amplitude
#
# =============================================================================
#


class TimeSlideBin(object):
	def __init__(self):
		self.livetime = 0.0
		self.amplitudes = []


class RateVsAmplitude(SnglBurstUtils.BurstPlot):
	def __init__(self, instrument, amplitude_factor):
		SnglBurstUtils.BurstPlot.__init__(self, r"%s Amplitude (\(\mathrm{s}^{-\frac{1}{3}}\))" % instrument, "Coincident Event Rate (Hz)", width = 108.0)
		self.instrument = instrument
		self.amplitude_factor = amplitude_factor
		self.time_slide_bins = {}
		self.foreground = []
		self.background = []
		self.foreground_time = 0.0
		self.background_time = 0.0
		self.axes.loglog()
		self.axes.set_position([0.125, 0.15, 0.83, 0.75])

	def add_contents(self, contents):
		zero_lag_time_slides, background_time_slides = SnglBurstUtils.get_time_slides(contents.connection)
		time_slide_index = {}
		for time_slide_id, offsets in background_time_slides.items():
			key = frozenset(offsets.items())
			if key not in self.time_slide_bins:
				self.time_slide_bins[key] = TimeSlideBin()
			self.time_slide_bins[key].livetime += SnglBurstUtils.time_slides_livetime(contents.seglists, [offsets])
			time_slide_index[time_slide_id] = self.time_slide_bins[key]
		if len(self.time_slide_bins) != len(background_time_slides):
			# FIXME:  this is here because I'm not convinced
			# that the procedure used above for mapping offset
			# vectors back to "experiments" is 100% reliable
			# (using frozenset(offsets.items()) as an immutable
			# key), so as a safety check I impose the
			# constraint that the number of time slides can't
			# change from one file to the next.  this is an
			# arbitrary restriction that happens to match how
			# the pipeline currently works.
			raise ValueError, "internal time slide offset vector representation mismatch"
		self.foreground_time += SnglBurstUtils.time_slides_livetime(contents.seglists, zero_lag_time_slides.values())
		self.background_time += SnglBurstUtils.time_slides_livetime(contents.seglists, background_time_slides.values())

		for amplitude, time_slide_id, is_background in contents.connection.cursor().execute("""
SELECT
	sngl_burst.amplitude,
	coinc_event.time_slide_id,
	EXISTS (
		SELECT
			*
		FROM
			time_slide
		WHERE
			time_slide.time_slide_id == coinc_event.time_slide_id
			AND time_slide.offset != 0
	)
FROM
	coinc_event
	JOIN coinc_event_map ON (
		coinc_event_map.coinc_event_id == coinc_event.coinc_event_id
	)
	JOIN sngl_burst ON (
		coinc_event_map.table_name == 'sngl_burst'
		AND coinc_event_map.event_id == sngl_burst.event_id
	)
WHERE
	sngl_burst.ifo == ?
	AND coinc_event.coinc_def_id == ?
		""", (self.instrument, contents.bb_definer_id)):
			if is_background:
				self.background.append(abs(amplitude) * self.amplitude_factor)
				time_slide_index[time_slide_id].amplitudes.append(abs(amplitude) * self.amplitude_factor)
			else:
				self.foreground.append(abs(amplitude) * self.amplitude_factor)

	def finish(self):
		print "%s zero-lag events: %d" % (self.instrument, len(self.foreground))
		print "Total time in %s zero-lag segments: %s s" % (self.instrument, str(self.foreground_time))
		print "%s time-slide events: %d" % (self.instrument, len(self.background))
		print "Total time in %s time-slide segments: %s s" % (self.instrument, str(self.background_time))
		self.axes.set_title(r"Cumulative Coincident Event Rate vs.\ Amplitude in %s" % self.instrument)
		self.background.sort()
		self.foreground.sort()
		self.background = numpy.array(self.background, dtype = "double")
		self.foreground = numpy.array(self.foreground, dtype = "double")
		print "Amplitude of loudest %s zero-lag survivor: %.9g s^(-1/3)" % (self.instrument, self.foreground[-1])
		# number expected in zero-lag and \sqrt{N} std dev
		background_y = numpy.arange(len(self.background), 0.0, -1.0, dtype = "double") / self.background_time * self.foreground_time
		background_yerr = numpy.sqrt(background_y)
		# convert to rate and uncertainty expected in zero-lag
		background_y /= self.foreground_time
		background_yerr /= self.foreground_time
		# rate observed in zero-lag
		foreground_y = numpy.arange(len(self.foreground), 0.0, -1.0, dtype = "double") / self.foreground_time

		# plot

		crimson_shower_plot = False
		if crimson_shower_plot:
			for time_slide in self.time_slide_bins.values():
				time_slide.amplitudes.sort()
				time_slide.amplitudes = numpy.array(time_slide.amplitudes, dtype = "double")
				self.axes.plot(time_slide.amplitudes, numpy.arange(len(time_slide.amplitudes), 0.0, -1.0, dtype = "double") / time_slide.livetime, "r-", alpha = 0.3)
			variance = numpy.zeros(len(self.background), dtype = "double")
			for i, (amplitude, mean_rate) in enumerate(zip(self.background, background_y)):
				variance[i] = sum(((time_slide.amplitudes >= amplitude).sum() / time_slide.livetime * self.foreground_time - mean_rate * self.foreground_time)**2 for time_slide in self.time_slide_bins.values()) / len(self.time_slide_bins)

		# warning:  the error bar polygon is not *really* clipped
		# to the axes' bounding box, the result will be incorrect
		# if the number of sample points is small.
		xmin = 5e-21
		xmax = 1e-19
		ymin = 1e-7
		ymax = 1e-4
		poly_x = numpy.concatenate((self.background, self.background[::-1]))
		poly_y = numpy.concatenate((background_y + 1 * background_yerr, (background_y - 1 * background_yerr)[::-1]))
		self.axes.add_patch(patches.Polygon(zip(poly_x, numpy.clip(poly_y, ymin, ymax)), edgecolor = "k", facecolor = "k", alpha = 0.3))
		line1, = self.axes.plot(self.background.repeat(2)[:-1], background_y.repeat(2)[1:], color = "k", linestyle = "-")
		if crimson_shower_plot:
			self.axes.plot(self.background, background_y + numpy.sqrt(variance) / self.foreground_time, color = "k", linestyle = "-")
			self.axes.plot(self.background, background_y - numpy.sqrt(variance) / self.foreground_time, color = "k", linestyle = "-")
		#line2, = self.axes.plot(self.foreground.repeat(2)[:-1], foreground_y.repeat(2)[1:], color = "k", linestyle = "-")
		line2, = self.axes.plot(self.foreground, foreground_y, "ko", markeredgecolor = "k")

		self.axes.legend((line1, line2), (r"Time-slide events", r"Zero-lag events"), loc = "upper right")

		self.axes.set_xlim([xmin, xmax])
		self.axes.set_ylim([ymin, ymax])
		self.axes.xaxis.grid(True, which="minor")
		self.axes.yaxis.grid(True, which="minor")


#
# =============================================================================
#
#                                  Efficiency
#
# =============================================================================
#


def create_recovered_snr_view(connection, bb_coinc_def_id):
	# Create a temporary virtual table containing three columns:  the
	# simulation_id of an injection, the name of an instrument, and the
	# largest SNR at which that injection was recovered in that
	# instrument by a burst event that participates in a coincidence of
	# type bb_coinc_def_id.
	#
	# NOTE:  can't use parameters in views, so must construct a query
	# string directly with the coinc_def_id.
	connection.cursor().execute("""
CREATE TEMPORARY VIEW
	recovered_snr
AS
	SELECT
		sim_burst.simulation_id AS simulation_id,
		sngl_burst.ifo AS ifo,
		MAX(ABS(sngl_burst.amplitude)) AS snr
	FROM
		sim_burst
		JOIN coinc_event_map AS a ON (
			a.table_name == "sim_burst"
			AND a.event_id == sim_burst.simulation_id
		)
		JOIN coinc_event_map AS b ON (
			b.coinc_event_id == a.coinc_event_id
		)
		JOIN sngl_burst ON (
			b.table_name == "sngl_burst"
			AND b.event_id == sngl_burst.event_id
		)
		JOIN coinc_event_map AS c ON (
			c.table_name == "sngl_burst"
			AND c.event_id == b.event_id
		)
		JOIN coinc_event ON (
			coinc_event.coinc_event_id == c.coinc_event_id
		)
	WHERE
		coinc_event.coinc_def_id == "%s"
	GROUP BY
		sim_burst.simulation_id,
		sngl_burst.ifo
	""" % bb_coinc_def_id)


def slope(x, y):
	"""
	From the x and y arrays, compute the slope at the x co-ordinates
	using a first-order finite difference approximation.
	"""
	slope = numpy.zeros((len(x),), dtype = "double")
	slope[0] = (y[1] - y[0]) / (x[1] - x[0])
	for i in xrange(1, len(x) - 1):
		slope[i] = (y[i + 1] - y[i - 1]) / (x[i + 1] - x[i - 1])
	slope[-1] = (y[-1] - y[-2]) / (x[-1] - x[-2])
	return slope


def upper_err(y, yerr, deltax):
	z = y + yerr
	deltax = int(deltax)
	upper = numpy.zeros((len(yerr),), dtype = "double")
	for i in xrange(len(yerr)):
		upper[i] = max(z[max(i - deltax, 0) : min(i + deltax, len(z))])
	return upper - y


def lower_err(y, yerr, deltax):
	z = y - yerr
	deltax = int(deltax)
	lower = numpy.zeros((len(yerr),), dtype = "double")
	for i in xrange(len(yerr)):
		lower[i] = min(z[max(i - deltax, 0) : min(i + deltax, len(z))])
	return y - lower


def write_efficiency(fileobj, bins, eff, yerr, filterwidth):
	print >>fileobj, "# ln(A)	e	D[e]"
	DlnA = bins[0].delta * filterwidth / 2.0
	for A, e, De in zip(bins.centres()[0], eff, yerr):
		print >>fileobj, math.log(A), e, De


def render_data_from_bins(dump_file, axes, efficiency, cal_uncertainty, filter_width, colour = "k", erroralpha = 0.3, linestyle = "-"):
	# extract array of x co-ordinates, and the factor by which x
	# increases from one sample to the next.
	(x,) = efficiency.centres()
	x_factor_per_sample = efficiency.bins()[0].delta

	# compute the efficiency, the slope (units = efficiency per
	# sample), the y uncertainty (units = efficiency) due to binomial
	# counting fluctuations, and the x uncertainty (units = samples)
	# due to the width of the smoothing filter.
	eff = efficiency.ratio()
	dydx = slope(numpy.arange(len(x), dtype = "double"), eff)
	yerr = numpy.sqrt(eff * (1 - eff) / efficiency.denominator.array)
	xerr = numpy.array([filter_width / 2] * len(yerr))

	# compute the net y err (units = efficiency) by (i) multiplying the
	# x err by the slope, (ii) dividing the calibration uncertainty
	# (units = percent) by the fractional change in x per sample and
	# multiplying by the slope, (iii) adding the two in quadradure with
	# the y err.
	net_yerr = numpy.sqrt((xerr * dydx)**2 + yerr**2 + (cal_uncertainty / x_factor_per_sample * dydx)**2)

	# compute net xerr (units = percent) by dividing yerr by slope and
	# then multiplying by the fractional change in x per sample.
	net_xerr = net_yerr / dydx * x_factor_per_sample

	# write the efficiency data to file
	write_efficiency(dump_file, efficiency.bins(), eff, net_yerr, filter_width)

	# plot the efficiency curve and uncertainty region
	patch = patches.Polygon(zip(numpy.concatenate((x, x[::-1])), numpy.concatenate((eff + upper_err(eff, yerr, filter_width / 2), (eff - lower_err(eff, yerr, filter_width / 2))[::-1]))), edgecolor = colour, facecolor = colour, alpha = erroralpha)
	axes.add_patch(patch)
	line, = axes.plot(x, eff, colour + linestyle)

	# compute 50% point and its uncertainty
	A50 = optimize.bisect(interpolate.interp1d(x, eff - 0.5), x[0], x[-1], xtol = 1e-40)
	A50_err = interpolate.interp1d(x, net_xerr)(A50)

	# mark 50% point on graph
	axes.axvline(A50, color = colour, linestyle = linestyle)

	# print some analysis
	num_injections = efficiency.denominator.array.sum()
	num_samples = len(efficiency.denominator)
	print "Bins were %g samples wide, ideal would have been %g" % (filter_width, (num_samples / num_injections / interpolate.interp1d(x, dydx)(A50)**2.0)**(1.0/3.0))
	print "Average number of injections in each bin = %g" % efficiency.denominator.array.mean()

	return line, A50, A50_err


class Efficiency(SnglBurstUtils.BurstPlot):
	def __init__(self, instruments, loudest_survivor, cal_uncertainty, amplitude_factor, filter_width):
		SnglBurstUtils.BurstPlot.__init__(self, r"Injection Amplitude (\(\mathrm{s}^{-\frac{1}{3}}\))", "Detection Efficiency", width = 108.0)
		self.axes.set_title(r"Detection Efficiency vs.\ Amplitude")
		self.instruments = instruments
		[(self.loudest_survivor_inst, self.loudest_survivor_ampl)] = loudest_survivor.items()
		self.cal_uncertainty = cal_uncertainty
		self.amplitude_factor = amplitude_factor
		self.width = filter_width
		self.found = []
		self.found_above_loudest = []
		self.all = []
		self.axes.semilogx()
		self.axes.set_position([0.10, 0.150, 0.86, 0.77])

		# set desired yticks
		self.axes.set_yticks((0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))
		self.axes.set_yticklabels((r"\(0\)", r"\(0.1\)", r"\(0.2\)", r"\(0.3\)", r"\(0.4\)", r"\(0.5\)", r"\(0.6\)", r"\(0.7\)", r"\(0.8\)", r"\(0.9\)", r"\(1.0\)"))

	def add_contents(self, contents):
		# for each injection, retrieve the highest SNR of the burst
		# events that were found to match the injection and that
		# participated in burst<-->burst coincidences or null if no
		# such matching burst events were found
		create_recovered_snr_view(contents.connection, contents.bb_definer_id)
		for values in contents.connection.cursor().execute("""
SELECT
	sim_burst.*,
	recovered_snr.snr
FROM
	sim_burst
	LEFT JOIN recovered_snr ON (
		recovered_snr.simulation_id == sim_burst.simulation_id
		AND recovered_snr.ifo == ?
	)
		""", (self.loudest_survivor_inst,)):
			sim = contents.sim_burst_table.row_from_cols(values[:-1])
			snr = values[-1]
			found = snr is not None
			if SimBurstUtils.injection_was_made(sim, contents.seglists, self.instruments):
				self.all.append(sim)
				if found:
					self.found.append(sim)
					if snr * self.amplitude_factor > self.loudest_survivor_ampl:
						self.found_above_loudest.append(sim)
			elif found:
				print >>sys.stderr, "odd, injection %s was found but not injected ..." % sim.simulation_id

	def finish(self):
		# put made and found injections in the denominators and
		# numerators of the efficiency bins
		bins = rate.NDBins((rate.LogarithmicBins(min(sim.amplitude for sim in self.all) * self.amplitude_factor, max(sim.amplitude for sim in self.all) * self.amplitude_factor, 400),))
		efficiency = rate.BinnedRatios(bins)
		efficiency_above_loudest = rate.BinnedRatios(bins)
		for sim in self.found:
			efficiency.incnumerator((sim.amplitude * self.amplitude_factor,))
		for sim in self.found_above_loudest:
			efficiency_above_loudest.incnumerator((sim.amplitude * self.amplitude_factor,))
		for sim in self.all:
			efficiency.incdenominator((sim.amplitude * self.amplitude_factor,))
			efficiency_above_loudest.incdenominator((sim.amplitude * self.amplitude_factor,))

		# regularize:  adjust unused bins so that the efficiency
		# ratio comes out to 0
		efficiency.regularize()
		efficiency_above_loudest.regularize()

		# generate and plot trend curves.  adjust window function
		# normalization so that denominator array correctly
		# represents the number of injections contributing to each
		# bin:  make w(0) = 1.0.  note that this factor has no
		# effect on the efficiency because it is common to the
		# numerator and denominator arrays.  we do this for the
		# purpose of computing the Poisson error bars, which
		# requires us to know the counts for the bins
		windowfunc = rate.gaussian_window(self.width)
		windowfunc /= windowfunc[len(windowfunc) / 2 + 1]
		rate.filter_binned_ratios(efficiency, windowfunc)
		rate.filter_binned_ratios(efficiency_above_loudest, windowfunc)

		line1, A50, A50_err = render_data_from_bins(file("string_efficiency.dat", "w"), self.axes, efficiency, self.cal_uncertainty, self.width, colour = "k", linestyle = "--", erroralpha = 0.2)
		print "Pipeline's 50%% efficiency point for all detections = %g +/- %g%%\n" % (A50, A50_err * 100)

		line2, A50_above_loudest, A50_above_loudest_err = render_data_from_bins(file("string_efficiency_above_loudest.dat", "w"), self.axes, efficiency_above_loudest, self.cal_uncertainty, self.width, colour = "k", linestyle = "-")
		print "Pipeline's 50%% efficiency point for detections above loudest = %g +/- %g%%\n" % (A50_above_loudest, A50_above_loudest_err * 100)

		# add a legend to the axes
		self.axes.legend((line1, line2), (r"All recovered injections", r"\noindent Injections recovered with\\\(A > %s\,\mathrm{s}^{-\frac{1}{3}}\) in %s" % (SnglBurstUtils.latexnumber("%.2g" % self.loudest_survivor_ampl), self.loudest_survivor_inst)), loc = "lower right")

		# adjust limits
		self.axes.set_xlim([4e-21, 1e-17])
		self.axes.set_ylim([0.0, 1.0])


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


options, filenames = parse_command_line()

if options.injections:
	plots = [
		Efficiency(("H1", "H2", "L1"), options.loudest_survivor, options.cal_uncertainty, options.amplitude_factor, options.injections_bin_size)
	]
	survivors = None
else:
	plots = [
		RateVsAmplitude("H1", options.amplitude_factor),
		RateVsAmplitude("H2", options.amplitude_factor),
		RateVsAmplitude("L1", options.amplitude_factor),
	]
	survivors = Survivors()


for n, filename in enumerate(filenames):
	if options.verbose:
		print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
	connection = sqlite3.connect(working_filename)
	dbtables.DBTable_set_connection(connection)
	database = SnglBurstUtils.CoincDatabase(connection, options.live_time_program, search = "StringCusp", verbose = options.verbose)
	for n, plot in enumerate(plots):
		if options.verbose:
			print >>sys.stderr, "adding to plot %d ..." % n
		plot.add_contents(database)
	if survivors is not None:
		if options.verbose:
			print >>sys.stderr, "storing zero-lag survivors ..."
		survivors.add_contents(database)
	connection.close()
	dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)

n = 0
format = "%%s%%0%dd.%%s" % (int(math.log10(max(len(plots) - 1, 1))) + 1)
while plots:
	if options.verbose:
		print >>sys.stderr, "finishing plot %d ..." % n
	plots[0].finish()
	for ext in options.image_formats:
		if options.injections:
			filename = format % ("string_injections_", n, ext)
		else:
			filename = format % ("string_triggers_", n, ext)
		if options.verbose:
			print >>sys.stderr, "writing %s ..." % filename
		plots[0].fig.savefig(filename)
	del plots[0]
	n += 1
if survivors:
	survivors.finish("string_triggers_survivors.xml", verbose = options.verbose)
if options.verbose:
	print >>sys.stderr, "done."
