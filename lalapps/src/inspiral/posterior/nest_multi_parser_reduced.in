#!/usr/bin/env @PYTHONPROG@

# DAG generation code for running inspnest pipeline
# (C) 2010 John Veitch, Salvatore Vitale <salvatore.vitale@ligo.org>

from lalapps import nest_utils as OddsPipeline
import ConfigParser
import os
import sys
from optparse import OptionParser,OptionValueError
import glue
from glue import pipeline
from glue import segmentsUtils
from glue import segments
from glue import segmentdb
from lalapps import inspiralutils
import scipy
sys.path.append('@PYTHONLIBDIR@')
import numpy as np
#Presets
ifos=['H1','L1','V1']
fakeTypes=["LALVirgo","LALLIGO","LALAdLIGO","LALAdVirgo"]

# Read command line options
usage=""" %prog [options]
Setup a Condor DAG file to run the nested sampling pipeline over multiple ini files.

"""

## This function is used to put the ifos.keys() in the same order than in the ifos section in the parser, while passing them to the inspnest nodes
def sort_ifo(a,b):
    global ifos
    if ifos.index(a)<ifos.index(b):
        return -1
    elif ifos.index(a)>ifos.index(b):
        return 1
    else: 
        return 0


def checkDir(dir):
        if not os.access(dir,os.F_OK):
                try:
                        print 'Creating %s\n'%(dir)
                        os.makedirs(dir)
                except:
                        print 'Error: Unable to create directory %s'%(dir)
                        sys.exit(1)
        if not os.access(dir,os.W_OK):
                print 'Unable to write to %s'%(dir)
                sys.exit(1)
        return True

# Simple function because lambda form doesn't work in python 2.4
def istoint(a):
    if a is not None:
        return 1
    else:
        return 0
def vararg_callback(option, opt_str, value, parser):
    assert value is None
    value = []
    def floatable(str):
        try:
            float(str)
            return True
        except ValueError:
            return False
    for arg in parser.rargs:
        # stop on --foo like options
        if arg[:2] == "--" and len(arg) > 2:
            break
        # stop on -a, but not on -3 or -3.0
        if arg[:1] == "-" and len(arg) > 1 and not floatable(arg):
            break
        value.append(arg)
    del parser.rargs[:len(value)]
    setattr(parser.values, option.dest, value)        
    
def nest_pipe(opts=None,num=0):
    
    global dag
    global dagfile
    global glob_combine_nodes_by_time
    global glob_combine_folders_by_num
    global glob_snrs_folders_by_num
    global glob_times
    global glob_IFOs
    global fnames_dic
    test_hyp=num # This is the hypotheses being tested with this subdag
    
    cp=ConfigParser.ConfigParser()
    cp.optionxform = str
    cp.readfp(open(opts.ini_file))
    if cp==None or opts==None:
        print "cp or opts are none... exiting \n"
        sys.exit(1)

    ifos=eval(cp.get('analysis','ifos'))
    print 'Setting up pipeline with ifos '+' '.join(map(str,ifos))

    IFOs=''.join(map(str,ifos))

    # Check command line options
    if opts.ini_file is None:
            parser.error("Must specify an ini file using --ini-file")
    #if opts.trigger_time is None:
    #       parser.error("Must specify the trigger time using --trigger-time")

    checkDir(opts.run_path)
    #if opts.log_path is None:
    #        opts.log_path=os.path.join(opts.run_path,"log")
    #checkDir(opts.log_path)

    # Get science segments    
    dt=float(cp.get('inspnest','dt'))


    if(sum(map(istoint, [getattr(opts,a) for a in ['single_triggers','coinc_triggers','inj','gps_time_file']]))>1):
        parser.error('You can only specify one of --inj, --single-triggers, --coinc-triggers, or --gps-time-file\n')

    types_list=eval(cp.get('data','types'))
    channels_list=eval(cp.get('data','channels'))


    #Create types and channels dictionaries
    types={}
    channels={}
    for ifo,type,channel in zip(ifos,types_list,channels_list):
        types[ifo]=type
        channels[ifo]=channel

    for ifo,channel in channels.items():
        cp.set('data',ifo.lower()+'-channel',channel)

        
    time_event=None
    if opts.inj and cp.has_option('analysis','events'):
        time_event={}
        events=[]
        times=[]
        raw_events=cp.get('analysis','events').replace('[','').replace(']','').split(',')

        from pylal import SimInspiralUtils
        injTable=SimInspiralUtils.ReadSimInspiralFromFiles([opts.inj])
        if 'all' is raw_events or 'all' in raw_events:
            events=range(len(injTable))

        else:
            for raw_event in raw_events:
                if ':' in raw_event:
                    limits=raw_event.split(':')
                    if len(limits) != 2:
                        print "Error: in event config option; ':' must separate two numbers."
                        exit(0)
                    low=int(limits[0])
                    high=int(limits[1])+1   # The plus one is to include the rigthmost extremum
                    if low>high:
                        events.extend(range(int(high),int(low)))
                    elif high>low:
                        events.extend(range(int(low),int(high)))
                else:
                    events.append(int(raw_event))

        for event in events:
            times.append(injTable[event].get_end())
            time_event[times[-1]]=event
            

        starttime=min(times)-1.0
        endtime=max(times)+1.0

        cp.set('results','injXML',opts.inj)

    elif opts.inj:
        from pylal import SimInspiralUtils
        injTable=SimInspiralUtils.ReadSimInspiralFromFiles([opts.inj])
        times=[inj.get_end() for inj in injTable]
        starttime=min(times)
        endtime=max(times)

    if opts.gps_time_file:
        import re
        p=re.compile('[\d.]+')
        times=[]
        timefile=open(opts.gps_time_file,'r')
        for time in timefile:
            if not p.match(time):
                continue
            if float(time) in times:
                print 'Skipping duplicate time %s'%(time)
                continue
            print 'Read time %s'%(time)
            times.append(float(time))
        timefile.close()
        starttime=min(times)
        endtime=max(times)

    if opts.single_triggers:
        from pylal import SnglInspiralUtils
        trigTable=SnglInspiralUtils.ReadSnglInspiralFromFiles([opts.single_triggers])
        times=[trig.get_end() for trig in trigTable]
        starttime=min(times)
        endtime=max(times)

    if opts.coinc_triggers:
        from pylal.xlal.datatypes.ligotimegps import LIGOTimeGPS
        from glue.ligolw import table
        from glue.ligolw import lsctables
        from glue.ligolw import utils

        CoincInspiralTriggers=None
        doc = utils.load_filename(opts.coinc_triggers, gz=(opts.coinc_triggers or "stdin").endswith(".gz"), verbose=False)
        # extract the sim inspiral table
        try: CoincInspiralTable = \
            table.get_table(doc, lsctables.CoincInspiralTable.tableName)
        except: CoincInspiralTable = None
        if CoincInspiralTriggers and CoincInspiralTable: 
            CoincInspiralTriggers.extend(CoincInspiralTable)
        elif not CoincInspiralTriggers:
            CoincInspiralTriggers = CoincInspiralTable

        times=[trig.get_end() for trig in CoincInspiralTriggers]
        starttime=min(times)
        endtime=max(times)

    if opts.gps_start_time is not None:
        mystarttime=float(opts.gps_start_time)
        if opts.inj:
            starttime=max([starttime,mystarttime])
            times=[time for time in times if time>starttime]
        else:
            starttime=mystarttime
        cp.set('analysis','gps-start-time',str(starttime))


    if opts.gps_end_time is not None:
        myendtime=float(opts.gps_end_time)
        if opts.inj:
            endtime=min([endtime,myendtime])
            times=[time for time in times if time<endtime]
        else:
            endtime=myendtime
        cp.set('analysis','gps-end-time',str(endtime))

    if not opts.inj and not opts.coinc_triggers and not opts.gps_time_file and not opts.single_triggers:
        if opts.gps_start_time is None and opts.gps_end_time is None:

            raise OptionValueError('Must specify both a start and end time if not using injection file or coinc file.\n')


        times=scipy.linspace(float(starttime)+dt/2.0,float(endtime)-dt/2.0,(float(endtime)-float(starttime))/dt)

    padding=float(cp.get('analysis','padding'))
    length=float(endtime)-float(starttime)+2*padding

    datastart=starttime-padding
    dataend=endtime+padding

    cp.add_section('input')
    cp.set('input','gps-start-time',str(int(datastart)))
    cp.set('input','gps-end-time',str(int(dataend)))


    print 'Setting up analysis for %i times between %.3f and %.3f\n'%(len(times),starttime,endtime)

    cache_dir=os.path.join(opts.run_path,'data')
    checkDir(cache_dir)
    segdir=os.path.join(opts.run_path,'segments')
    checkDir(segdir)
    os.chdir(segdir)
    science_segs={}
    seg_files={}
    segs={}

    basename="nest_%.3f-%.3f"%(starttime,endtime)
    
    # Create DAG #################
    ###logfile=os.path.join(opts.log_path,basename+'.log')
    '''if num==0:
        glob_IFOs=ifos
        glob_times=times
        dagfile=os.path.join(opts.run_path,"..",basename)
        dag = pipeline.CondorDAG(logfile)
    dag.set_dag_file(dagfile)
    '''
    datafind_job = pipeline.LSCDataFindJob(cache_dir,opts.jobs_log_path,cp)
    datafind_job.add_opt('url-type','file')
    datafind_job.set_sub_file(os.path.join(opts.run_path,'datafind.sub'))

    # Build list of science segments
    # Covers entire time range. Not necessarily all used
    for ifo in ifos:
        if not opts.ignore_science_mode and types[ifo] not in fakeTypes:
            seg_files[ifo]=inspiralutils.science_segments(ifo,cp)
            segfile=open(seg_files[ifo])
            #segs[ifo]=segmentsUtils.fromfilenames([seg_files[ifo]])
            segs[ifo]=segmentsUtils.fromsegwizard(segfile)
            segs[ifo].coalesce()
            segfile.close()
        else:   # If we skip the segdb step, just construct a large segment
            print 'Faking segment from %i to %i\n'%(datastart,dataend)
            segs[ifo]=segments.segmentlist([segments.segment(int(datastart),int(dataend))])


    for ifo in ifos:
        science_segs[ifo]=[]
        if types[ifo] in fakeTypes:
            science_segs[ifo].append(None)
        else:
            # Setup find data jobs
            for seg in segs[ifo]:
                sciseg=pipeline.ScienceSegment((segs[ifo].index(seg),seg[0],seg[1],seg[1]-seg[0]))
                science_segs[ifo].append(sciseg)
                df_node=pipeline.LSCDataFindNode(datafind_job)
                df_node.set_start(int(sciseg.start()))
                df_node.set_end(int(sciseg.end()))
                df_node.set_observatory(ifo[0])
                df_node.set_type(types[ifo])
                sciseg.set_df_node(df_node)

    os.chdir('../../')

    # Now loop over times and add datafind nodes to the dag

    filtered_time=filter(lambda t: reduce(lambda a,b:a or b, map(lambda ifo: t in segs[ifo],ifos)), times)
    times=filtered_time
    print 'Found segments for %i times\n'%(len(times))
    if num==fnames_dic[0]:
        for time in times:
            glob_parents_by_time_by_hyp[str(time)]={}
            
    df_nodes_by_time={}
    ifos_by_time={}
    segments_by_time={}
    #if num==0:
    #    for time in times:
    #        glob_posteriors[time]={}

    for time in times:
        ifos_by_time[time]=[]
        segments_by_time[time]={}
        for ifo in ifos:
            for (seg,sciseg) in zip(segs[ifo],science_segs[ifo]):
                if time in seg:
                    segments_by_time[time][ifo]=(seg,sciseg)
                #else:
                #   segments_by_time[time][ifo]=None
            #if segments_by_time[time][ifo] is None:
            #   continue
                    ifos_by_time[time].append(ifo)
                    if segments_by_time[time][ifo][1] is not None:
                        node=segments_by_time[time][ifo][1].get_df_node()
                        if node not in dag.get_nodes():
                            dag.add_node(node)

    # Filter out times with no data
    #times=[time for time in times if len(segments_by_time[time].keys())>0 ]
    print 'Found data for %i times\n'%(len(times))

    if len(times)==0:
        print 'Unable to find any science data for times specified. Aborting!'
        sys.exit(1)

    inspnest_subfile=os.path.join(opts.run_path,'inspnest.sub')
    inspnest_job=OddsPipeline.InspNestJob(cp,inspnest_subfile,opts.jobs_log_path)
    inspnest_job.add_opt('Nlive',cp.get('analysis','nlive'))
    inspnest_job.add_opt('Nmcmc',cp.get('analysis','nmcmc'))
    #option for phitest
    if cp.has_option('inspnest','phaseParamTest'):
        inspnest_job.add_opt('phaseParamTest',cp.get('inspnest','phaseParamTest'))
    if cp.has_option('analysis','data_seed'):
        print "Using dataseed= " + str(cp.get('analysis','data_seed')) +" for the first injection. Incrementing by one for each other \n"
    else:
        print "Using default dataseed=1234 for the first injection (incrementing by one for each other). Add data_seed=X in the analysis section of the parser if you want to change it. \n"

    if cp.has_option('analysis','seed'):
        print "Using  seed= " + str(cp.get('analysis','seed')) +" for the fist run (seed is incremented by one for each parallel run) \n"
    else:
        print "Using default seed=100. Add seed=X in the analysis section of the parser file to use a different value \n"

    for option in cp.items('inspnest_all'):
        inspnest_job.add_opt(option[0],option[1])

    if opts.inj and not opts.disable_inject:
        inspnest_job.add_opt('inj',opts.inj)

    nest_path=os.path.join(opts.run_path,'nest')
    checkDir(nest_path)

    combine_dir=os.path.join(opts.run_path,'combine')
    snr_dir=os.path.join(opts.run_path,'SNR')
    glob_combine_folders_by_num[num]=combine_dir
    glob_snrs_folders_by_num[num]=snr_dir
    glob_combine_folders_by_num[num]=combine_dir
    if cp.has_section('calibration'):
        if cp.has_option('calibration','enable-calfreq'):
            inspnest_job.add_opt('enable-calfreq','')
            calerr_dir=os.path.join(opts.run_path,'CalErrors')
            checkDir(calerr_dir)
            glob_caler_folders_by_num[num]=calerr_dir
            inspnest_job.add_opt('calib-errors-path',calerr_dir)
            if cp.has_option('calibration','calib-seed'):
                #inspnest_job.add_opt('calib-seed',cp.get('calibration','calib-seed'))
                calib_seed=int(cp.get('calibration','calib-seed'))
            else:
                calib_seed=0
            checkDir(calerr_dir)
        else:
            print "The parser %s has a calibration section but it does not contain an enable-calfreq=1 line. Exiting..."%opts.ini_file
            sys.exit(1)
            
    checkDir(snr_dir)
    inspnest_job.add_opt('snrpath',snr_dir)
    time_combine_sub_file=os.path.join(opts.run_path,'combine_each_time.sub')
    combine_each_time_job=OddsPipeline.CombineZJob(cp,time_combine_sub_file,opts.jobs_log_path)
    combine_each_time_nodes={}

    inspnest_nodes={}
    inspnest_nodes_by_time={}
    nest_files={}
    # Now loop over times and set up combine jobs for getting individual posteriors
    nparallel=int(cp.get('analysis','nparallel'))
    posfiles_by_time={}
    bayesfiles_by_time={}
    snrs_by_time={}
    for time in times:
        posfile=os.path.join(combine_dir,'posterior_samples_%.3f'%(time))
        posfiles_by_time[time]=posfile
        #glob_posteriors[time][num]=posfile
        combine_node=OddsPipeline.CombineZNode(combine_each_time_job)
        combine_node.add_file_opt('outpos',posfile,file_is_output_file=True)
        bayesfiles_by_time[time]=os.path.join(combine_dir,'bayesfactor_%.3f.txt'%(time))
        snrs_by_time[time]=os.path.join(snr_dir,'snr_%s_%9.1f.dat'%(IFOs,time))
        combine_node.add_file_opt('bayesfactor',bayesfiles_by_time[time])
        combine_each_time_nodes[time]=combine_node
    # Set up inspnest jobs, using parallel if needed



    if nparallel==1:
        for time in times:
            if time_event is not None and not opts.disable_inject:
                    thisevent=time_event[time]
            else:
                    thisevent=None
            inspnest_node=OddsPipeline.setup_single_nest(cp,inspnest_job,time,segments_by_time[time],nest_path,ifos=sorted(segments_by_time[time].keys(),sort_ifo),event=thisevent)
            inspnest_node.set_priority(int(-round(40*(float(times.index(time))/float(len(times))-0.5))))
            inspnest_nodes_by_time[time]=inspnest_node
            if cp.has_section('calibration'):
                inspnest_node.add_var_opt("calib-seed",str(int(calib_seed+11*times.index(time))))
            dag.add_node(inspnest_node)
            combine_each_time_nodes[time].add_parent(inspnest_node)
            nest_files[time]=inspnest_node.get_output_files()[0]
            glob_parents_by_time_by_hyp[str(time)][str(test_hyp)]=inspnest_node
    else:
        merge_sub_file=os.path.join(opts.run_path,'merge.sub')
        merge_job=OddsPipeline.MergeJob(cp,merge_sub_file,opts.jobs_log_path)
        for time in times:
            if time_event is not None and not opts.disable_inject:
                    thisevent=time_event[time]
            else:
                    thisevent=None
            (merge_node,inspnest_nodes)=OddsPipeline.setup_parallel_nest(cp,inspnest_job,merge_job,time,segments_by_time[time],nest_path,ifos=sorted(segments_by_time[time].keys(),sort_ifo),event=thisevent)
            dag.add_node(merge_node)
            map(lambda a:a.set_priority(int(-round(40*(float(times.index(time))/float(len(times))-0.5)))) ,inspnest_nodes)
            map(dag.add_node,inspnest_nodes)
            nest_files[time]=merge_node.get_output_files()[0]
            combine_each_time_nodes[time].add_parent(merge_node)
            inspnest_nodes_by_time[time]=merge_node

    #for time in times:
    #    dag.add_node(combine_each_time_nodes[time])
    #    combine_each_time_nodes[time].add_var_arg(nest_files[time])
    #    combine_each_time_nodes[time].add_file_opt('headers',nest_files[time]+'_params.txt')
    #    glob_combine_nodes_by_time.append(combine_each_time_nodes[time])
    # Set up coherence test jobs if required
    if opts.coherence_test:
        coherence_test_sub_file=os.path.join(opts.run_path,'coherence_test.sub')
        coherence_test_job=OddsPipeline.CoherenceTestJob(cp,coherence_test_sub_file,opts.jobs_log_path)
        coherence_inspnest_nodes={}
        coherence_test_posfiles={}
        coherence_test_nodes={}
        coherence_test_files={}
        coherence_test_outfiles={}
        coherence_test_bfiles={}
        combine_nodes_by_time={}
        snr_files_by_time={}
        coherence_test_dir=os.path.join(opts.run_path,'coherencetest')
        checkDir(coherence_test_dir)
        # Loop over all the times
        for time in times:
            ifos=segments_by_time[time].keys()
            if len(ifos)<2:
                print 'Unable to setup coherence test for time %.3f, only %s data available.\n'%(time,ifos[0])
                continue
            coherence_test_node=OddsPipeline.CoherenceTestNode(coherence_test_job)
            coherence_test_node.add_var_arg(nest_files[time])
            coherence_test_files[time]={}
            coherence_test_posfiles[time]={}
            coherence_test_bfiles[time]={}
            coherence_inspnest_nodes[time]={}
            combine_nodes_by_time[time]={}
            snr_files_by_time[time]={}
            for ifo in segments_by_time[time].keys():
                combine_node=OddsPipeline.CombineZNode(combine_each_time_job)
                combine_nodes_by_time[time][ifo]=combine_node
                coherence_test_posfiles[time][ifo]=os.path.join(combine_dir,'posfile_%.3f_%s.dat'%(time,ifo))
                coherence_test_bfiles[time][ifo]=os.path.join(combine_dir,'bayesfactor_%.3f_%s.txt'%(time,ifo))
                snr_files_by_time[time][ifo]=os.path.join(snr_dir,'snr_%s_%9.1f.dat'%(ifo,time))
                combine_node.add_file_opt('bayesfactor',coherence_test_bfiles[time][ifo],file_is_output_file=True),
                combine_node.add_file_opt('outpos',coherence_test_posfiles[time][ifo],file_is_output_file=True)
                if time_event is not None and not opts.disable_inject:
                    thisevent=time_event[time]
                else:
                    thisevent=None
                if nparallel>1:
                    (merge_node,inspnest_nodes)=OddsPipeline.setup_parallel_nest(cp,inspnest_job,merge_job,time,segments_by_time[time],nest_path,ifos=[ifo],event=thisevent)
                    dag.add_node(merge_node)
                    coherence_test_files[time][ifo]=merge_node.get_output_files()[0]
                    #combine_node.add_parent(merge_node)
                    #coherence_test_node.add_parent(merge_node)
                    coherence_inspnest_nodes[time][ifo]=merge_node
                else:
                    inspnest_node=OddsPipeline.setup_single_nest(cp,inspnest_job,time,segments_by_time[time],nest_path,ifos=[ifo],event=thisevent)
                    inspnest_nodes=[inspnest_node]
                    coherence_test_files[time][ifo]=inspnest_node.get_output_files()[0]
                    #combine_node.add_parent(inspnest_node)
                    coherence_inspnest_nodes[time][ifo]=inspnest_node
                combine_node.add_var_arg(coherence_test_files[time][ifo])
                combine_node.add_file_opt('headers',coherence_test_files[time][ifo]+'_params.txt')
                coherence_test_node.add_var_arg(coherence_test_files[time][ifo])
                #map(coherence_test_node.add_parent, inspnest_nodes)
                map(dag.add_node,inspnest_nodes)
                #dag.add_node(combine_node)
            coherence_test_outfiles[time]=os.path.join(coherence_test_dir,'coherence_test_%.3f.txt'%(time))
            coherence_test_node.add_file_opt('outfile',coherence_test_outfiles[time],file_is_output_file=True)
            #coherence_test_node.add_parent(inspnest_nodes_by_time[time])
            coherence_test_nodes[time]=coherence_test_node
            dag.add_node(coherence_test_node)

    times_by_seg={}
    seg_block={}
    inspnest_by_seg={}
    nestfiles_by_seg={}
    co_nests_by_seg={}
    co_nestfiles_by_seg={}
    # Also want to combine all jobs within each segment
    for time in times:
        ifos=segments_by_time[time].keys()
        smallest_segment=segments.segment(max([segments_by_time[time][ifo][0][0] for ifo in ifos]), min([segments_by_time[time][ifo][0][1] for ifo in ifos]))
    #   smallest_segment=reduce(lambda a,b:a and b, [segments_by_time[time][ifo][0] for ifo in segments_by_time[time].keys()])
        if smallest_segment not in times_by_seg.keys():
            times_by_seg[smallest_segment]=[]
            inspnest_by_seg[smallest_segment]=[]
            nestfiles_by_seg[smallest_segment]=[]
            co_nests_by_seg[smallest_segment]={}
            co_nestfiles_by_seg[smallest_segment]={}
            if len(segments_by_time[time].keys())>1:
                for ifo in segments_by_time[time].keys():
                    co_nests_by_seg[smallest_segment][ifo]=[]
                    co_nestfiles_by_seg[smallest_segment][ifo]=[]
        times_by_seg[smallest_segment].append(time)
        inspnest_by_seg[smallest_segment].append(inspnest_nodes_by_time[time])
        nestfiles_by_seg[smallest_segment].append(nest_files[time])
        # Append the coherence test files
        if opts.coherence_test and len(segments_by_time[time].keys())>1:
            for ifo in segments_by_time[time].keys():
                co_nests_by_seg[smallest_segment][ifo].append(coherence_inspnest_nodes[time][ifo])
                co_nestfiles_by_seg[smallest_segment][ifo].append(coherence_test_files[time][ifo])

    print 'Broke into %i chunks for recombination'%(len(times_by_seg))

    combine_sub_file=os.path.join(opts.run_path,'combine.sub')
    combine_job=OddsPipeline.CombineZJob(cp,combine_sub_file,opts.jobs_log_path)
    combine_nodes={}
    combine_dir=os.path.join(opts.run_path,'combine')
    checkDir(combine_dir)

    if opts.disable_pages is False:
        # Set up web page producing nodes
        results_page_sub_file=os.path.join(opts.run_path,'results_page.sub')
        results_page_job=OddsPipeline.ResultsPageJob(cp,results_page_sub_file,opts.jobs_log_path)
        if(len(times)==1):
            results_path=os.path.join(cp.get('results','basedir'),str(time))
            glob_timebins_folder_by_num[num]=os.path.join(results_path,'timebins')
        else:
            results_path=os.path.join(cp.get('results','basedir'),'%.3f-%.3f'%(min(times),max(times)))
            glob_timebins_folder_by_num[num]=os.path.join(results_path,'timebins')
        if(opts.notify is not None):
            results_page_job.set_notification('Complete')
            results_page_job.add_condor_cmd('notify_user',opts.notify)

    # Analyse each segment

    for seg in times_by_seg.keys():
        #combine_node=OddsPipeline.CombineZNode(combine_job)
        posfile=os.path.join(combine_dir,'posterior_samples_%.3f-%.3f.dat'%(seg[0],seg[1]))
        bffile=os.path.join(combine_dir,'bayesfactor_%.3f-%.3f.txt'%(seg[0],seg[1]))
        #combine_node.add_file_opt('outpos',posfile)
        #combine_node.add_file_opt('bayesfactor',bffile)
        outsamp=os.path.join(combine_dir,'nested_samples_%.3f-%.3f.dat'%(seg[0],seg[1]))
        #combine_node.add_file_opt('outsamp',outsamp)
        #map(combine_node.add_var_arg,nestfiles_by_seg[seg])
        #map(lambda a: combine_node.add_file_opt('headers',a+'_params.txt'),nestfiles_by_seg[seg])
        #map(combine_node.add_parent,inspnest_by_seg[seg])
        #dag.add_node(combine_node)
        #combine_nodes[seg]=combine_node
        
        if opts.coherence_test and len(co_nests_by_seg[seg].keys())>1:
            co_combine_node={}
            co_pos_file={}
            co_bf_file={}
            co_outsamp={}
            for ifo in co_nests_by_seg[seg].keys():
                co_combine_node[ifo]=OddsPipeline.CombineZNode(combine_job)
                co_pos_file[ifo]=posfile.replace('.dat','_'+ifo+'.dat')
                co_bf_file[ifo]=bffile.replace('.txt','_'+ifo+'.txt')
                co_outsamp[ifo]=outsamp.replace('.dat','_'+ifo+'.dat')
                co_combine_node[ifo].add_file_opt('outpos',co_pos_file[ifo],file_is_output_file=True)
                co_combine_node[ifo].add_file_opt('bayesfactor',co_bf_file[ifo],file_is_output_file=True)
                co_combine_node[ifo].add_file_opt('outsamp',co_outsamp[ifo],file_is_output_file=True)
                map(co_combine_node[ifo].add_parent,co_nests_by_seg[seg][ifo])
                map(lambda a:co_combine_node[ifo].add_file_opt('headers',a+'_params.txt'),co_nestfiles_by_seg[seg][ifo])
                map(co_combine_node[ifo].add_var_arg,co_nestfiles_by_seg[seg][ifo])
                dag.add_node(co_combine_node[ifo])
            coherence_test_node=OddsPipeline.CoherenceTestNode(coherence_test_job)
            cotest_outfile_seg=os.path.join(coherence_test_dir,'coherence_test_%.3f-%.3f.txt'%(seg[0],seg[1]))
            coherence_test_node.add_var_opt('outfile',cotest_outfile_seg)
            coherence_test_node.add_var_arg(outsamp)
            map(coherence_test_node.add_var_arg, co_outsamp.values())
            map(coherence_test_node.add_parent, co_combine_node.values())
            coherence_test_node.add_parent(combine_node)
            dag.add_node(coherence_test_node)
            # Generate results page for each IFO too
            if opts.disable_pages is False:
                for ifo in co_nests_by_seg[seg].keys():
                    results_node=OddsPipeline.ResultsPageNode(results_page_job)
                    results_node.add_file_opt('data',co_pos_file[ifo])
                    results_node.add_parent(co_combine_node[ifo])
                    seg_results_path=os.path.join(results_path,'%.3f-%.3f'%(seg[0],seg[1]),ifo)
                    results_node.add_var_opt('outpath',seg_results_path)
                    results_node.add_var_opt('bsn',co_bf_file[ifo])
                    dag.add_node(results_node)


        if opts.disable_pages is False:
            # Produce web page output for each segment
            results_node=OddsPipeline.ResultsPageNode(results_page_job)
            results_node.add_file_opt('data',posfile)
            seg_results_path=os.path.join(results_path,'%.3f-%.3f'%(seg[0],seg[1]))
            checkDir(seg_results_path)
            results_node.add_var_opt('outpath',seg_results_path)
            results_node.add_var_opt('bsn',bffile)
            if opts.coherence_test and len(co_nests_by_seg[seg].keys())>1:
                results_node.add_parent(coherence_test_node)
                results_node.add_var_arg('--bci '+cotest_outfile_seg)
            results_node.add_parent(combine_node)
            dag.add_node(results_node)

    if opts.enable_pages_alltimes is True:
        # Also produce plots for each time chunk
        for time in times:
            results_page_node=OddsPipeline.ResultsPageNode(results_page_job)
            results_page_node.add_file_opt('data',posfiles_by_time[time])
            results_dir=os.path.join(results_path,'timebins',str(time))
            checkDir(results_dir)
            results_page_node.add_var_opt('outpath',results_dir)
            if time_event is not None:   
                if time_event[time] is not None:
                    results_page_node.set_event_number(time_event[time])
                    results_page_node.add_var_arg('--inj '+os.path.abspath(opts.inj))
            results_page_node.add_parent(combine_each_time_nodes[time])
            results_page_node.add_var_opt('bsn',bayesfiles_by_time[time])
            results_page_node.add_var_arg('--snr '+snrs_by_time[time])

            dag.add_node(results_page_node)

            if opts.coherence_test and (time in coherence_inspnest_nodes.keys()):
                results_page_node.add_var_arg('--bci '+coherence_test_outfiles[time])
                results_page_node.add_parent(coherence_test_nodes[time])
                if opts.coherence_test and (time in coherence_inspnest_nodes.keys()):
                    for ifo in coherence_inspnest_nodes[time].keys():
                        results_node=OddsPipeline.ResultsPageNode(results_page_job)
                        results_node.add_file_opt('data',coherence_test_posfiles[time][ifo])
                        if time_event is not None:   
                            if time_event[time] is not None:
                                results_node.set_event_number(time_event[time])
                                results_node.add_var_arg('--inj '+os.path.abspath(opts.inj))
                        results_node.add_parent(combine_nodes_by_time[time][ifo])
                        ifo_results_path=os.path.join(results_dir,ifo)
                        results_node.add_var_opt('outpath',ifo_results_path)
                        results_node.add_var_opt('bsn',coherence_test_bfiles[time][ifo])
                        results_node.add_var_arg('--snr '+snr_files_by_time[time][ifo])
                    
                        dag.add_node(results_node)
    


parser=OptionParser(usage)
parser.add_option("-i","--ini-file",dest="ini_file",default=None, action="callback", callback=vararg_callback,help="ini-file for nestGRB pipeline",metavar="CONFIG.ini")
parser.add_option("-r","--run-path",default="./",action="store",type="string",help="Directory to run pipeline in (default: %default)",metavar="RUNDIR")
parser.add_option("-o","--compare-path",default="./",action="store",type="string",help="Directory where comparison pages and data are stored (default: %default)",metavar="COMPAREDIR")
parser.add_option("-p","--dag-log-path",default=None,action="store",type="string",help="Directory to store dag log files, defaults to RUNDIR/log/",metavar="LOGDIR")
parser.add_option("-l","--jobs-log-path",default=None,action="store",type="string",help="Directory to store jobs  log files, defaults to RUNDIR/log/",metavar="LOGDIR")
parser.add_option("-s","--gps-start-time",action="store",type="string",default=None,help="Start time of analysis")
parser.add_option("-e","--gps-end-time",action="store",type="string",default=None,help="End time of analysis")
parser.add_option("-I","--inj",action="store",type="string",default=None,help="List of injections to perform and analyse",metavar="INJFILE.xml")
parser.add_option("-F","--disable-inject",action="store_true",default=False,help="Don't actually inject the signals - useful for hardware injection lists")
parser.add_option("-t","--single-triggers",action="store",type="string",default=None,help="SnglInspiralTable trigger list",metavar="SNGL_FILE.xml")
parser.add_option("-T","--coinc-triggers",action="store",type="string",default=None,help="CoincInspiralTable trigger list",metavar="COINC_FILE.xml")
parser.add_option("-g","--gps-time-file",action="store",type="string",default=None,help="Text file containing list of GPS times to analyse",metavar="TIMES.txt")
parser.add_option("-C","--coherence-test",action="store_true",help="Perform the coherence test",default=False)
parser.add_option("-P","--folder-names",dest="fnames",action="callback", callback=vararg_callback,help="These are the folders that will be created, corresponding to the parameters that are pinned or GR",default=None,metavar="KeySection")
parser.add_option("--disable-pages",action="store_true",default=True,help="Disable the outpage page and postprocessing")
parser.add_option("--enable-pages-alltimes",action="store_true",default=False,help="Enable a results page for every time bin. May create a lot of dirs.")
parser.add_option("--condor-submit",action="store_true",default=False,help="Automatically submit the condor dag")
parser.add_option("--ignore-science-mode",action="store_true",default=False,help="Skip the segment database step and try and find data for all specified times.")
parser.add_option("--notify",action="store",default=None,help="Optional e-mail address to notify upon completion",metavar="you@yourdomain.edu")
parser.add_option("-R","--remote-script",default=None,action="store",type="string",help="The path to the script which safely append to the remote database",metavar="svitale@login.nikhef.nl:/project/gravwav/safe_append.sh")
parser.add_option("-D","--remote-database",default=None,action="store",type="string",help="The name of the remote database is stored",metavar="database.txt")
parser.add_option("-S","--testParam-ShiftPc",default=None,action="store",type="string",help="The shifted parameter underscore the percent value of the shift, or GR", metavar="dphi6_1pc")
parser.add_option("-Q","--inspinj-seed",default=None,action="store",type="string",help="The unique value of the inspinj seed", metavar="700000")

(opts,args)=parser.parse_args()
opts.disable_pages=True
inits=opts.ini_file
num_of_inits=len(opts.ini_file)
fnames=opts.fnames
num_of_fnames=len(opts.fnames)



if not num_of_inits==num_of_fnames:
    print "You seem to be using %d parser files and %d foldernames. These two numbers must be the same. Exiting...\n"
    sys.exit(1)

fnames_dic={}
for fname in fnames:
    fnames_dic[fnames.index(fname)]=str(fname)

glob_hyp=opts.fnames
hyp_str=" "

for hy in glob_hyp:
    hyp_str+=hy+" "

common_path=opts.run_path

if (opts.dag_log_path!="None" and opts.dag_log_path is not None):
    common_dag_log_path=opts.dag_log_path
else:
    common_dag_log_path=os.path.join(common_path,'logs')
checkDir(common_dag_log_path)

if (opts.jobs_log_path is not None and opts.jobs_log_path!="None"):
    common_jobs_log_path=opts.jobs_log_path
else:
    common_jobs_log_path=os.path.join(common_path,'logs')
checkDir(common_jobs_log_path)

glob_combine_nodes_by_time=[]
glob_snrs_folders_by_num={}
glob_combine_folders_by_num={}
glob_timebins_folder_by_num={}
glob_caler_folders_by_num={}
has_key_section={}
glob_parents_by_time_by_hyp={}

daglogfile=os.path.join(common_dag_log_path,'common_dag.log')
dagfile=os.path.join(common_path,"common_dag")
dag = pipeline.CondorDAG(daglogfile)
dag.set_dag_file(dagfile)

for init in inits:
    if init is not None:
        cp=ConfigParser.ConfigParser()
        cp.optionxform = str
        cp.readfp(open(init))
        opts.ini_file=inits[inits.index(init)]
        opts.run_path=os.path.join(str(common_path),str(fnames_dic[inits.index(init)]))
        opts.jobs_log_path=os.path.join(common_jobs_log_path,str(fnames_dic[inits.index(init)]),"logs")
        checkDir(opts.jobs_log_path)
        nest_pipe(opts,str(fnames_dic[inits.index(init)]))

want_database=0

if opts.remote_script is not None:
    want_database+=1
if opts.remote_database is not None:
    want_database+=1
if opts.testParam_ShiftPc is not None:
    want_database+=1
if opts.inspinj_seed is not None:
    want_database+=1

if want_database==4:

    clusters={"pcdev1.phys.uwm.edu":"Nemo","marlin.phys.uwm.edu":"Marlin","hydra.phys.uwm.edu":"Hydra","atlas1.atlas.aei.uni-hannover.de":"Atlas1","atlas2.atlas.aei.uni-hannover.de":"Atlas2","atlas3.atlas.aei.uni-hannover.de":"Atlas3","atlas4.atlas.aei.uni-hannover.de":"Atlas4","titan1.atlas.aei.uni-hannover.de":"Titan1","titan2.atlas.aei.uni-hannover.de":"Titan2","titan3.atlas.aei.uni-hannover.de":"Titan3","ldas-grid.ligo.caltech.edu":"Cit","sugar.phy.sys.edu":"Sugar","ldas-grid.ligo-la.caltech.edu":"LLO","ldas-grid.ligo-wa.caltech.edu":"LHO1","ldas-pcdev1.ligo-wa.caltech.edu":"LHO2"}
    for un in os.uname():
        if un in clusters.keys():
            local_pc=clusters[un]
            break
        else:
            local_pc=os.uname()[1]

    database_subfile=os.path.join(common_path,'database.sub')
    database_job=OddsPipeline.DataBaseJob(cp,database_subfile,common_jobs_log_path)

    for time in glob_parents_by_time_by_hyp.keys():
        database_node=OddsPipeline.DataBaseNode(database_job)
        map(database_node.add_parent,glob_parents_by_time_by_hyp[time].values())
        database_node.set_time(time)
        database_node.add_var_opt("path",common_path)
        database_node.add_var_opt("subhyp", hyp_str)
        database_node.add_var_opt("remote-script",opts.remote_script)
        database_node.add_var_opt("remote-database",opts.remote_database)
        database_node.add_var_opt("cluster",local_pc)
        database_node.add_var_opt("testParam-ShiftPc",opts.testParam_ShiftPc)
        database_node.add_var_opt("inspinj-seed",opts.inspinj_seed)
        
        dag.add_node(database_node)
    print "Switching on database synchronization."
elif want_database==3:
    print "WARNING: You seem to be giving only three among the options -R, -D, -S and -Q. If you want to sincronize your results with the remote database you need to provide all these three options"
    print "Switching off database synchronization"
elif want_database==2:
    print "WARNING: You seem to be giving only two among the options -R, -D, -S and -Q. If you want to sincronize your results with the remote database you need to provide all these three options"
    print "Switching off database synchronization"
elif want_database==1:
    print "WARNING: You seem to be giving only one among the options -R, -D, -S and -Q. If you want to sincronize your results with the remote database you need to provide all these three options"
    print "Switching off database synchronization"
else:
    print "Switching off database synchronization"
    
# Write the DAG file
dag.write_sub_files()
dag.write_dag()
dag.write_script()

# End of program
print 'Successfully created DAG file.'
print 'Now run condor_submit_dag %s\n'%(dag.get_dag_file())

if opts.condor_submit:
    import subprocess
    from subprocess import Popen
    
    x = subprocess.Popen(['condor_submit_dag',dag.get_dag_file()])
    x.wait()
    if x.returncode==0:
        print 'Submitted DAG file'
    else:
        print 'Unable to submit DAG file'
